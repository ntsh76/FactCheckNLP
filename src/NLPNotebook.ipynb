{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d8ff1d",
   "metadata": {},
   "source": [
    "#### Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c345234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install tensorflow\n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install evaluate\n",
    "#!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bac89c",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d73c1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hugging face imports\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import create_optimizer\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should point to FactCheckNLPApp/\n",
    "BASE_PATH = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4697bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Download data\n",
    "\n",
    "- This uses data from https://github.com/neemakot/Health-Fact-Checking.\n",
    "- First clone this repo in your local workspace. \n",
    "- Then run download_data script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5fb318aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !../Health-Fact-Checking/src/download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e70c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193425ca",
   "metadata": {},
   "source": [
    "#### Preprocess data and select topk sentences from main text\n",
    "\n",
    "- This can take a few hours on a single GPU \n",
    "- Skip this step and download files directly if you don't want to change pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fact_check_nlp.preprocessing import select_evidence_sentences_based_on_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbd7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/neeteshtiwari/Documents/FactCheckNLPApp/Health-Fact-Checking/data/PUBHEALTH/formatted_train.csv\n",
      "Writing to /Users/neeteshtiwari/Documents/FactCheckNLPApp/Health-Fact-Checking/data/PUBHEALTH/formatted_dev.csv\n",
      "Writing to /Users/neeteshtiwari/Documents/FactCheckNLPApp/Health-Fact-Checking/data/PUBHEALTH/formatted_test.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/train.tsv'\n",
    "dev_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/dev.tsv'\n",
    "test_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/test.tsv'\n",
    "\n",
    "select_evidence_sentences_based_on_cosine_similarity(train_path, k=5, \n",
    "                                output_path=train_path.replace('train.tsv', 'formatted_train.csv')\n",
    "                               )\n",
    "\n",
    "select_evidence_sentences_based_on_cosine_similarity(dev_path, k=5, \n",
    "                                output_path=dev_path.replace('dev.tsv', 'formatted_dev.csv')\n",
    "                               )\n",
    "\n",
    "select_evidence_sentences_based_on_cosine_similarity(test_path, k=5, \n",
    "                                output_path=test_path.replace('test.tsv', 'formatted_test.csv')\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffccb77",
   "metadata": {},
   "source": [
    "#### Training \n",
    "\n",
    "- Current this trains a hugging face dummy model just to see if everything works\n",
    "- It basically only claim text and labels to call fit and then predict\n",
    "- Results will not make sense at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b51b3a",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfc03127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a3d2975df6d9a990\n",
      "Found cached dataset csv (/Users/neeteshtiwari/.cache/huggingface/datasets/csv/default-a3d2975df6d9a990/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016211986541748047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9b0a76f2ee441188876bd13b20f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-552ef83724876e0d\n",
      "Found cached dataset csv (/Users/neeteshtiwari/.cache/huggingface/datasets/csv/default-552ef83724876e0d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018798828125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee162383e1b8472a91fb6df86be8fdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a675004b814f61b4\n",
      "Found cached dataset csv (/Users/neeteshtiwari/.cache/huggingface/datasets/csv/default-a675004b814f61b4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020554304122924805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6c47d9081d4b9b91059af8d0ff49d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_file = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_train.csv' \n",
    "val_file = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_dev.csv' \n",
    "test_file = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_test.csv' \n",
    "\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=[train_file])\n",
    "val_dataset = load_dataset(\"csv\", data_files=[val_file])\n",
    "test_dataset = load_dataset(\"csv\", data_files=[test_file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631b363",
   "metadata": {},
   "source": [
    "#### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d4e7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"true\", 1: \"false\", 2: \"mixture\", 3: \"unproven\"}\n",
    "label2id = {\"true\": 0, \"false\": 2, \"mixture\":3, \"unproven\": 4}\n",
    "\n",
    "def preprocess_function(examples, col1='claim', col2='top_k', labels=[]):    \n",
    "    inputs = tokenizer(\n",
    "        examples[col1],\n",
    "        examples[col2],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    inputs['label'] = examples['label']\n",
    "    return inputs\n",
    "\n",
    "def preprocess_claim(examples):\n",
    "    inputs = tokenizer(examples[\"claim\"], truncation=True)\n",
    "\n",
    "    # print(np.unique(examples['label']))\n",
    "    converted_lbls = []\n",
    "    for val in examples['label']:\n",
    "        converted_lbls.append(label2id.get(val, -2))\n",
    "\n",
    "    # print(np.unique(converted_lbls))    \n",
    "    inputs['label'] = converted_lbls\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72ee1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006766796112060547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6e63f1e73a4bfdba34bcbf7108ae23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007195234298706055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da3faa7695843df8226964f40376a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007389068603515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 83,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f5cada364d47b19438de2a2c643c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_train = dataset.map(preprocess_claim, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_val = val_dataset.map(preprocess_claim, batched=True, remove_columns=val_dataset[\"train\"].column_names)\n",
    "tokenized_test = test_dataset.map(preprocess_claim, batched=True, remove_columns=test_dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a99a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b4ba9",
   "metadata": {},
   "source": [
    "#### Load Pre-trained Model & Traing it only on Claim texts just to check if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be7828e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_data = tokenizer(dataset[\"train\"]['claim'], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = []\n",
    "for lbl in dataset[\"train\"]['label']:\n",
    "    labels.append(label2id.get(lbl, 6))\n",
    "labels = np.array(labels)  # Label is already an array of 0 and 1\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08944621",
   "metadata": {},
   "source": [
    "#### Predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4004803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7489774823188782},\n",
       " {'label': 'LABEL_0', 'score': 0.7489068508148193},\n",
       " {'label': 'LABEL_0', 'score': 0.7489771246910095},\n",
       " {'label': 'LABEL_0', 'score': 0.7485695481300354},\n",
       " {'label': 'LABEL_0', 'score': 0.749027669429779},\n",
       " {'label': 'LABEL_0', 'score': 0.7485504746437073},\n",
       " {'label': 'LABEL_0', 'score': 0.7491540908813477},\n",
       " {'label': 'LABEL_0', 'score': 0.7491301894187927},\n",
       " {'label': 'LABEL_0', 'score': 0.7490165829658508},\n",
       " {'label': 'LABEL_0', 'score': 0.7491080164909363}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(test_dataset[\"train\"][\"claim\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c0a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
