{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMKQdFcWmlGL"
   },
   "source": [
    "### Claim Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapted from DistillBert example on HuggingFace website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuMlXT80GAMK"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQMKTZ4ARk12"
   },
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    \n",
    "device = mps_device    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT0NcXTLmlGP"
   },
   "source": [
    "<a id='section02'></a>\n",
    "### Importing and Pre-Processing the domain data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should point to FactCheckNLPApp/\n",
    "BASE_PATH = './'\n",
    "LABEL_TO_ID = {\"true\": 0, \"false\": 1, \"mixture\": 1, \"unproven\": 1}\n",
    "NUM_CLASSES = 2\n",
    "CLAIM_feature = 'claim'\n",
    "\n",
    "train_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_train_most_similar.csv'\n",
    "dev_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_dev_most_similar.csv'\n",
    "test_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/formatted_test_most_similar.csv'\n",
    "FEATURES = ['claim','top_k', 'label']\n",
    "EVIDENCE_FEATURE = 'top_k'\n",
    "\n",
    "'''\n",
    "train_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/train_claim_sentence_pair.csv'\n",
    "dev_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/dev_claim_sentence_pair.csv'\n",
    "test_path = f'{BASE_PATH}Health-Fact-Checking/data/PUBHEALTH/test_claim_sentence_pair.csv'\n",
    "FEATURES = ['claim', 'evidence_sentence', 'label']\n",
    "\n",
    "EVIDENCE_FEATURE = 'evidence_sentence'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_some_random_train_data():    \n",
    "    lst = []\n",
    "    lst.append({\n",
    "        'claim': \"earth is round.\",\n",
    "        'top_k': \"earth is round studies have proven. Flat earthers have give up. It has been proven scientifically. There is no doubt about this fact. Several studies have confirmed. Satellites rely on the roundness of earth. It has been established beyond doubt that earth is spherical.\",\n",
    "        'label': 0\n",
    "    })\n",
    "    lst.append({\n",
    "        'claim': \"There are 365 days in a year.\",\n",
    "        'top_k': \"One year has 365 days is a known fact. This doesnot require any proof. It has always been True.\",\n",
    "        'label': 0\n",
    "    })\n",
    "    lst.append({\n",
    "        'claim': \"Being healthy is good for the human body\",\n",
    "        'top_k': \"It is true that Being healthy is good for the human body. Running/Walking or any form of exercise really good for you. As it promotes good health.\",\n",
    "        'label': 0\n",
    "    })\n",
    "    lst.append({\n",
    "        'claim': \"Excessive sugar consumption is bad for you.\",\n",
    "        'top_k': \"Studies have proven that too much sugar consumption is not good for you. It can cause weight gain. It can also cause other health issues. Avoiding sugar is good. Studies have proven that excessive sugar consumption is bad for your healthy.\",\n",
    "        'label': 0\n",
    "    })\n",
    "    \n",
    "    lst.append({\n",
    "        'claim': \"Drinking water is good for you.\",\n",
    "        'top_k': \"It is true that drinking water is good for you. All published research indicates that drinking water is good for you.\",\n",
    "        'label': 0\n",
    "    })\n",
    "\n",
    "    lst.append({\n",
    "        'claim': \"This claim is True.\",\n",
    "        'top_k': \"This claim is not wrong. This claim is right. This claim is correct. This claim has been proven correct.\",\n",
    "        'label': 0\n",
    "    })\n",
    "\n",
    "    lst.append({\n",
    "        'claim': \"This claim is False.\",\n",
    "        'top_k': \"This claim is indeed incorrect. This claim is not right. This claim is frivolous. It is based on fabricated research. It is based on baseless claims.\",\n",
    "        'label': 0\n",
    "    })\n",
    "\n",
    "    # False claims\n",
    "    lst.append({\n",
    "        'claim': \"This claim is True.\",\n",
    "        'top_k': \"This claim is indeed incorrect. This claim is not right. This claim is frivolous. It is based on fabricated research. It is based on baseless claims.\",\n",
    "        'label': 1\n",
    "    })    \n",
    "    \n",
    "    lst.append({\n",
    "        'claim': \"Exercise is bad for you.\",\n",
    "        'top_k': \"Exercise is good for you. It has been proven mutliple times. All published studies indicate the benefits of exercise on health.\",\n",
    "        'label': 1\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(lst)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNCaZ2epNcSO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_df(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[FEATURES]\n",
    "    print(df['label'].unique())\n",
    "    # df = df[(df['label'].isin(LABEL_TO_ID.keys()))]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    encoded_lbls = []\n",
    "    for lbl in df['label'].values:\n",
    "        encoded_lbls.append(LABEL_TO_ID[lbl])\n",
    "    df['label'] = encoded_lbls\n",
    "    return df\n",
    "\n",
    "train_df = prepare_df(train_path)\n",
    "valid_df = prepare_df(dev_path)\n",
    "test_df = prepare_df(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.concat([train_df, add_some_random_train_data()]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNU33DMJmlGP"
   },
   "source": [
    "<a id='section03'></a>\n",
    "### Preparing the Dataset and Dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrBr2YesGdO_"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vX7kzaAHu39"
   },
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, claim_index_map):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.claim_index_map = claim_index_map\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        claim = self.data.claim[index]\n",
    "        evidence = self.data[EVIDENCE_FEATURE].values[index]\n",
    "\n",
    "        title = str(claim + \" \" + evidence)\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'claim_index': self.claim_index_map[claim],\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.label[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "def claim_index(df):\n",
    "    claim_idx_map = {}\n",
    "    vals = df['claim'].unique()\n",
    "    idx = 0\n",
    "    for val in vals:\n",
    "        claim_idx_map[val] = idx\n",
    "        idx += 1\n",
    "        \n",
    "    return claim_idx_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zcwq13c0NE9c",
    "outputId": "e1a2b0dc-4719-4b6a-ef71-e3232d86940b"
   },
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_dataset= train_df.reset_index(drop=True)\n",
    "train_claim_idx_map = claim_index(train_dataset)\n",
    "\n",
    "valid_dataset= valid_df.reset_index(drop=True)\n",
    "valid_claim_idx_map = claim_index(valid_dataset)\n",
    "\n",
    "test_dataset= test_df.reset_index(drop=True)\n",
    "test_claim_idx_map = claim_index(test_dataset)\n",
    "\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN, train_claim_idx_map)\n",
    "valid_set = Triage(valid_dataset, tokenizer, MAX_LEN, valid_claim_idx_map)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN, test_claim_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1BgA1CkQSYa"
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "valid_loader = DataLoader(valid_set, **test_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-yROwP4mlGR"
   },
   "source": [
    "<a id='section04'></a>\n",
    "### Creating the Neural Network for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxvJGstBmlGR"
   },
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51SHU41fmlGR",
    "outputId": "69d18ae2-7bac-4426-987e-a8e701c668e3"
   },
   "outputs": [],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ADa0qlBmlGR"
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXG-rFh9mlGS"
   },
   "source": [
    "<a id='section05'></a>\n",
    "### Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H579QAVmmlGS"
   },
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVGOI1M_mlGS"
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    idx = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    print(nb_tr_steps, nb_tr_examples)\n",
    "    print(tr_loss, n_correct)\n",
    "    \n",
    "    preds = []\n",
    "    lbls = []\n",
    "    claim_ids = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            nb_tr_steps += 1            \n",
    "            idx += 1\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            index = data['claim_index'].item()\n",
    "            claim_ids.append(index)\n",
    "            \n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            outputs = model(ids, mask).squeeze()\n",
    "\n",
    "            # print(outputs)\n",
    "            # print(targets)\n",
    "\n",
    "            # loss = loss_function(outputs, targets)\n",
    "            # tr_loss += loss.item()\n",
    "            prob = torch.max(outputs.data).item()\n",
    "            big_idx = torch.argmax(outputs.data) #, dim=1\n",
    "            n_correct += calcuate_accu(big_idx, targets)                \n",
    "\n",
    "            preds.append(big_idx.item())\n",
    "            lbls.append(targets.item())\n",
    "\n",
    "            if idx%1000 == 0:\n",
    "                # loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                # print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 1000 steps: {accu_step}\")\n",
    "                # break\n",
    "                \n",
    "    print(nb_tr_steps, nb_tr_examples)\n",
    "    # print(tr_loss, n_correct)\n",
    "    # epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    # print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu, preds, prob, lbls, claim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wnl4TMA1mlGS",
    "outputId": "f9fe0d27-bc77-4129-f4d8-61d248e88529"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    \n",
    "    train(epoch)\n",
    "    valid(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claim_text = \"Chocolates are sweet.\"\n",
    "evidence_text = \"Chocolates are sweet.\"\n",
    "\n",
    "claim_text = \"Chocolates are not sweet\"\n",
    "evidence_text = \"Chocolates are not sweet.\"\n",
    "\n",
    "claim_text = \"Chocolates are sweet\"\n",
    "evidence_text = \"Chocolates are sweet.\"\n",
    "\n",
    "claim_text = \"Humans are honest\"\n",
    "evidence_text = \"Humans are honest.\"\n",
    "\n",
    "claim_text = \"Sky is blue.\"\n",
    "evidence_text = \"Sky is blue\"\n",
    "\n",
    "## seems like period makes difference\n",
    "title = str(claim_text + \" \" + evidence_text)\n",
    "title = \" \".join(title.split())\n",
    "inputs = tokenizer.encode_plus(\n",
    "    title,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    pad_to_max_length=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True\n",
    ")\n",
    "ids = torch.tensor([inputs['input_ids']], dtype=torch.long).to(device)\n",
    "mask = torch.tensor([inputs['attention_mask']], dtype=torch.long).to(device)\n",
    "\n",
    "outputs = model(ids, mask).squeeze()\n",
    "prob = torch.max(outputs.data).item()\n",
    "big_idx = torch.argmax(outputs.data) #, dim=1\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EhaheVrmlGS"
   },
   "source": [
    "<a id='section06'></a>\n",
    "### Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIW6LulZmlGT",
    "outputId": "db45944b-ab87-438a-a445-4a4cff4c084a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "acc, preds, pred_prob, lbls, claim_ids = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "\n",
    "print(classification_report(y_true=lbls, y_pred=preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_max(vals):\n",
    "    lst = []\n",
    "    for val in vals:\n",
    "        lst.append(val.item())\n",
    "    return max(lst)\n",
    "\n",
    "def find_majority(vals):    \n",
    "    lst = list(vals)\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "def map_gt(gt):\n",
    "    lst = list(gt)\n",
    "    vals = []\n",
    "    for val in lst:\n",
    "        vals.append(LABEL_TO_ID[val])\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'claim_id': claim_ids,\n",
    "    'preds': preds,\n",
    "    'pred_prob': pred_prob,\n",
    "    'label': lbls\n",
    "})\n",
    "\n",
    "pred_df = pred_df.sort_values(by='claim_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.groupby(['claim_id', 'label']).max()\n",
    "\n",
    "pred_df['pred_prob'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df = pred_df.groupby(['claim_id', 'label'])['preds'].apply(lambda x : find_majority(x)).reset_index()\n",
    "\n",
    "new_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lbls = new_pred_df['label'].values\n",
    "preds = new_pred_df['preds'].values \n",
    "print(classification_report(lbls, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIKMnjN0mlGT"
   },
   "source": [
    "<a id='section07'></a>\n",
    "### Saving the Trained Model  for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDXjxNXLmlGT",
    "outputId": "1f78910c-d605-47e8-a911-e0a9caf3c391"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/tmp/claim_classification_true_false_distillbert-2epochs')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_transformers_multiclass_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
